import spaces
import torch
import gradio as gr
import yt_dlp as youtube_dl
from transformers import pipeline
from transformers.pipelines.audio_utils import ffmpeg_read
from transformers import WhisperProcessor
from pyannote.audio import Pipeline as PyannotePipeline
from datetime import datetime
import tempfile
import numpy as np
from itertools import groupby
from datetime import datetime
from gradio.components import State
import os
import re


    
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:307'

try:
    diarization_pipeline = PyannotePipeline.from_pretrained(
        "pyannote/speaker-diarization-3.1",
        use_auth_token=os.environ["HF_TOKEN"]
    )
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    diarization_pipeline.to(device)
except Exception as e:
    print(f"Error initializing diarization pipeline: {e}")
    diarization_pipeline = None

# Updated Whisper model
MODEL_NAME = "openai/whisper-medium"
#BATCH_SIZE = 2  # RÃ©duction de la taille du batch
FILE_LIMIT_MB = 1000
YT_LENGTH_LIMIT_S = 3600

device = 0 if torch.cuda.is_available() else "cpu"

pipe = pipeline(
    task="automatic-speech-recognition",
    model=MODEL_NAME,
    #chunk_length_s=30,
    device=device,
    model_kwargs={"low_cpu_mem_usage": True},
)




def associate_speakers_with_timestamps(transcription_result, diarization, tolerance=0.01, min_segment_duration=0.05):
    word_segments = transcription_result['chunks']
    diarization_segments = list(diarization.itertracks(yield_label=True))
    speaker_transcription = []
    current_speaker = None
    current_text = []
    unassigned_words = []
    last_segment_index = 0

    def flush_current_segment():
        nonlocal current_speaker, current_text
        if current_speaker and current_text:
            segment_duration = word_segments[-1]['timestamp'][1] - word_segments[0]['timestamp'][0]
            if segment_duration >= min_segment_duration:
                speaker_transcription.append((current_speaker, ' '.join(current_text)))
            else:
                unassigned_words.extend([(word['timestamp'][0], word['text']) for word in word_segments])
            current_text = []

    for word in word_segments:
        word_start, word_end = word['timestamp']
        word_text = word['text']
        assigned = False

        for i in range(last_segment_index, len(diarization_segments)):
            segment, _, speaker = diarization_segments[i]
            if segment.start - tolerance <= word_start < segment.end + tolerance:
                if speaker != current_speaker:
                    flush_current_segment()
                    current_speaker = speaker
                current_text.append(word_text)
                last_segment_index = i
                assigned = True
                break

        if not assigned:
            unassigned_words.append((word_start, word_text))

    flush_current_segment()

    # Traitement des mots non assignÃ©s
    unassigned_words.sort(key=lambda x: x[0])  # Trier par timestamp
    for word_start, word_text in unassigned_words:
        closest_segment = min(diarization_segments, key=lambda x: min(abs(x[0].start - word_start), abs(x[0].end - word_start)))
        speaker = closest_segment[2]
        if speaker != current_speaker:
            flush_current_segment()
            current_speaker = speaker
        current_text.append(word_text)
    flush_current_segment()

    # Fusion des segments courts
    merged_transcription = []
    for speaker, text in speaker_transcription:
        if not merged_transcription or merged_transcription[-1][0] != speaker:
            merged_transcription.append((speaker, text))
        else:
            merged_transcription[-1] = (speaker, merged_transcription[-1][1] + " " + text)

    return merged_transcription
    
def simplify_diarization_output(speaker_transcription):
    simplified = []
    for speaker, text in speaker_transcription:
        simplified.append(f"{speaker}: {text}")
    return "\n".join(simplified)

def parse_simplified_diarization(simplified_text):
    pattern = r"(SPEAKER_\d+):\s*(.*)"
    matches = re.findall(pattern, simplified_text, re.MULTILINE)
    return [(speaker, text.strip()) for speaker, text in matches]

def process_transcription(*args):
    generator = transcribe_and_diarize(*args)
    for progress_message, raw_text, speaker_transcription in generator:
        pass  # Consommer le gÃ©nÃ©rateur jusqu'Ã  la fin
    simplified_diarization = simplify_diarization_output(speaker_transcription)
    return progress_message, raw_text, simplified_diarization

def process_yt_transcription(*args):
    html_embed, raw_text, speaker_transcription = yt_transcribe(*args)
    simplified_diarization = simplify_diarization_output(speaker_transcription)
    return html_embed, raw_text, simplified_diarization
    

# New functions for progress indicator
def create_progress_indicator():
    return gr.State({"stage": 0, "message": "En attente de dÃ©marrage..."})

def update_progress(progress_state, stage, message):
    progress_state["stage"] = stage
    progress_state["message"] = message
    return progress_state

def display_progress(progress_state):
    stages = [
        "Chargement du fichier",
        "PrÃ©paration de l'audio",
        "Transcription en cours",
        "Diarisation (identification des locuteurs)",
        "Finalisation des rÃ©sultats"
    ]
    progress = (progress_state["stage"] / len(stages)) * 100
    return gr.HTML(f"""
        <style>
            @keyframes pulse {{
                0% {{ box-shadow: 0 0 0 0 rgba(66, 133, 244, 0.7); }}
                70% {{ box-shadow: 0 0 0 10px rgba(66, 133, 244, 0); }}
                100% {{ box-shadow: 0 0 0 0 rgba(66, 133, 244, 0); }}
            }}
        </style>
        <div style="margin-bottom: 20px; background-color: #f0f0f0; padding: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
            <h4 style="margin-bottom: 10px; color: #333; font-weight: bold;">Progression de la transcription</h4>
            <div style="background-color: #e0e0e0; height: 24px; border-radius: 12px; overflow: hidden; position: relative;">
                <div style="background-color: #4285F4; width: {progress}%; height: 100%; border-radius: 12px; transition: width 0.5s ease-in-out;"></div>
                <div style="position: absolute; top: 0; left: 0; right: 0; bottom: 0; display: flex; align-items: center; justify-content: center; color: #333; font-weight: bold;">
                    {progress:.0f}%
                </div>
            </div>
            <p style="margin-top: 10px; color: #666; font-style: italic;">{progress_state["message"]}</p>
            <div style="width: 10px; height: 10px; background-color: #4285F4; border-radius: 50%; margin-top: 10px; animation: pulse 2s infinite;"></div>
        </div>
    """)
    
@spaces.GPU(duration=120)
def transcribe_and_diarize(file_path, task, progress=gr.Progress()):
    progress(0, desc="Initialisation...")
    yield "Chargement du fichier...", None, None

    progress(0.2, desc="PrÃ©paration de l'audio...")
    yield "PrÃ©paration de l'audio...", None, None

    progress(0.4, desc="Laissez moi quelques minutes pour dÃ©chiffrer les voix et rÃ©diger l'audio ğŸ¤“ âœï¸ ...")
    transcription_result = pipe(file_path, generate_kwargs={"task": task, "language": "fr"}, return_timestamps="word")
    yield "Transcription en cours...", None, None

    progress(0.6, desc=" C'est fait ğŸ˜®â€ğŸ’¨ ! Je m'active Ã  fusionner tout Ã§a, un instant, J'y suis presque...")
    if diarization_pipeline:
        diarization = diarization_pipeline(file_path)
        speaker_transcription = associate_speakers_with_timestamps(transcription_result, diarization)
    else:
        speaker_transcription = [(None, transcription_result['text'])]
    yield "Diarisation en cours...", None, None

    progress(0.8, desc="Finalisation des rÃ©sultats...")
    yield "VoilÃ !", transcription_result['text'], speaker_transcription

    progress(1.0, desc="TerminÃ©!")
    return "Transcription terminÃ©e!", transcription_result['text'], speaker_transcription

def format_to_markdown(transcription_text, speaker_transcription, audio_duration=None, location=None, speaker_age=None, context=None):
    metadata = {
        "Date de traitement": datetime.now().strftime('%d/%m/%Y %H:%M'),
        "DurÃ©e de l'audio": f"{audio_duration} secondes" if audio_duration else "[Ã  remplir]",
        "Lieu": location if location else "[non spÃ©cifiÃ©]",
        "Ã‚ge de l'intervenant": f"{speaker_age} ans" if speaker_age else "[non spÃ©cifiÃ©]",
        "Contexte": context if context else "[non spÃ©cifiÃ©]"
    }
    
    metadata_text = "\n".join([f"- **{key}** : '{value}'" for key, value in metadata.items()])
    
    try:
        if isinstance(speaker_transcription, str):
            speaker_transcription = parse_simplified_diarization(speaker_transcription)
        
        if isinstance(speaker_transcription, list) and all(isinstance(item, tuple) and len(item) == 2 for item in speaker_transcription):
            formatted_transcription = []
            for speaker, text in speaker_transcription:
                formatted_transcription.append(f"**{speaker}**: {text}")
            transcription_text = "\n\n".join(formatted_transcription)
        else:
            raise ValueError("Invalid speaker transcription format")
    except Exception as e:
        print(f"Error formatting speaker transcription: {e}")
        transcription_text = "Error formatting speaker transcription. Using raw transcription instead.\n\n" + transcription_text

    formatted_output = f"""
# Transcription FormatÃ©e

## MÃ©tadonnÃ©es
{metadata_text}

## Transcription
{transcription_text}
"""
    return formatted_output

def _return_yt_html_embed(yt_url):
    video_id = yt_url.split("?v=")[-1]
    HTML_str = (
        f'<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/{video_id}"> </iframe>'
        " </center>"
    )
    return HTML_str

def download_yt_audio(yt_url, filename):
    info_loader = youtube_dl.YoutubeDL()
    
    try:
        info = info_loader.extract_info(yt_url, download=False)
    except youtube_dl.utils.DownloadError as err:
        raise gr.Error(str(err))
    
    file_length = info["duration"]
    
    if file_length > YT_LENGTH_LIMIT_S:
        yt_length_limit_hms = time.strftime("%H:%M:%S", time.gmtime(YT_LENGTH_LIMIT_S))
        file_length_hms = time.strftime("%H:%M:%S", time.gmtime(file_length))
        raise gr.Error(f"La durÃ©e maximale YouTube est de {yt_length_limit_hms}, la vidÃ©o YouTube dure {file_length_hms}.")
    
    ydl_opts = {"outtmpl": filename, "format": "bestaudio/best"}
    
    with youtube_dl.YoutubeDL(ydl_opts) as ydl:
        try:
            ydl.download([yt_url])
        except youtube_dl.utils.ExtractorError as err:
            raise gr.Error(str(err))

@spaces.GPU(duration=120)
def yt_transcribe(yt_url, task):
    html_embed_str = _return_yt_html_embed(yt_url)

    with tempfile.TemporaryDirectory() as tmpdirname:
        filepath = os.path.join(tmpdirname, "video.mp4")
        download_yt_audio(yt_url, filepath)
        with open(filepath, "rb") as f:
            inputs = f.read()

    inputs = ffmpeg_read(inputs, pipe.feature_extractor.sampling_rate)
    inputs = {"array": inputs, "sampling_rate": pipe.feature_extractor.sampling_rate}

    transcription_result = pipe(inputs, generate_kwargs={"task": task}, return_timestamps=True)
    transcription_text = transcription_result['text']

    if diarization_pipeline:
        diarization = diarization_pipeline(filepath)
        speaker_transcription = associate_speakers_with_timestamps(transcription_result, diarization)
    else:
        speaker_transcription = [(None, transcription_text)]

    return html_embed_str, transcription_text, speaker_transcription

def create_info_box(title, content):
    return gr.Markdown(f"### {title}\n{content}")

css_file_path = os.path.join(os.path.dirname(__file__), "style.css")
with open(css_file_path, "r") as f:
    custom_css = f.read()

theme = gr.themes.Default().set(
    body_background_fill="#f0f2f5",
    body_background_fill_dark="#2c3e50",
    button_primary_background_fill="#3498db",
    button_primary_background_fill_dark="#2980b9",
    button_secondary_background_fill="#2ecc71",
    button_secondary_background_fill_dark="#27ae60",
)

demo = gr.Blocks(
    theme=gr.themes.Default(),
    title="Scribe - Assistant de Transcription Audio ğŸ™ï¸ğŸ“",
    css=custom_css
)


with demo:
    gr.Markdown("""# ğŸ™ï¸ **Scribe** : L'assistant de Transcription Audio Intelligent ğŸ“ 
    ### âš ï¸ Cette version est une maquette publique. Ne pas mettre de donnÃ©es sensibles, privÃ©es ou confidentielles. âš ï¸""")
    gr.HTML(
        """
        <div class="logo">
            <img src="https://image.noelshack.com/fichiers/2024/33/4/1723713257-dbe58773-0638-445b-a88c-3fc1f2002408.jpg" alt="Scribe Logo">
        </div>
        """
    )
    gr.Markdown("## **Bienvenue sur Scribe, une solution pour la transcription audio sÃ©curisÃ©e. Transformez efficacement vos fichiers audio, enregistrements en direct ou vidÃ©os YouTube en texte prÃ©cis.**")

    gr.Markdown("""
    ### ğŸ” **Fonctionnement du ModÃ¨le** :
    Scribe utilise une approche en deux Ã©tapes pour transformer l'audio en texte structurÃ© :

    1. **Transcription avec Whisper Medium** :
       - ModÃ¨le de reconnaissance vocale dÃ©veloppÃ© par OpenAI
       - Utilise un rÃ©seau neuronal encodeur-dÃ©codeur avec attention
       - Capable de traiter divers accents et bruits de fond
       - OptimisÃ© pour un Ã©quilibre entre prÃ©cision et rapiditÃ©

    2. **Diarisation avec pyannote/speaker-diarization-3.1** :
       - Identifie et segmente les diffÃ©rents locuteurs dans l'audio
       - Utilise des techniques d'apprentissage profond pour l'extraction de caractÃ©ristiques vocales
       - Applique un algorithme de clustering pour regrouper les segments par locuteur



    ### ğŸ’¡ **Conseils pour de Meilleurs RÃ©sultats**
    - Utilisez des enregistrements de haute qualitÃ© avec peu de bruit de fond.
    - Pour les longs enregistrements, il est recommandÃ© de segmenter votre audio.
    - VÃ©rifiez toujours la transcription, en particulier pour les termes techniques ou les noms propres.
    - Utilisez des microphones externes pour les enregistrements en direct si possible.

    ### âš™ï¸ SpÃ©cifications Techniques :
    - ModÃ¨le de transcription : Whisper Medium
    - Pipeline de diarisation : pyannote/speaker-diarization-3.1
    - Limite de taille de fichier : _(Nous n'avons, Ã  ce jour, pas de limite prÃ©cise. Cependant, **nous vous recommandons de ne pas dÃ©passer 5 minutes.** )_
    - DurÃ©e maximale pour les vidÃ©os YouTube : _(Nous n'avons, Ã  ce jour, pas de limite prÃ©cise. Cependant, pour une utilisation optimale, l'audio ne doit pas dÃ©passer 30 minutes. )_
    - Formats audio supportÃ©s : MP3, WAV, M4A, et plus
    """)
    with gr.Accordion("ğŸ” SÃ©curitÃ© des DonnÃ©es et Pipelines", open=False):
        gr.Markdown("""

    #### Qu'est-ce qu'une pipeline ?
    Une pipeline dans le contexte de l'apprentissage automatique est une sÃ©rie d'Ã©tapes de traitement des donnÃ©es, allant de l'entrÃ©e brute Ã  la sortie finale. Dans Scribe, nous utilisons deux pipelines principales :

    1. **Pipeline de Transcription** : BasÃ©e sur le modÃ¨le Whisper Medium, elle convertit l'audio en texte.
    2. **Pipeline de Diarisation** : Identifie les diffÃ©rents locuteurs dans l'audio.

    #### Comment fonctionnent nos pipelines ?
    1. **Chargement Local** : Les modÃ¨les sont chargÃ©s localement sur votre machine ou serveur.
    2. **Traitement In-Situ** : Toutes les donnÃ©es sont traitÃ©es sur place, sans envoi Ã  des serveurs externes.
    3. **MÃ©moire Volatile** : Les donnÃ©es sont stockÃ©es temporairement en mÃ©moire vive et effacÃ©es aprÃ¨s utilisation.

    #### SÃ©curitÃ© et ConfidentialitÃ©
    - **Pas de Transmission Externe** : Vos donnÃ©es audio et texte restent sur votre systÃ¨me local.
    - **Isolation** : Chaque session utilisateur est isolÃ©e des autres.
    - **Nettoyage Automatique** : Les fichiers temporaires sont supprimÃ©s aprÃ¨s chaque utilisation.
    - **Mise Ã  Jour SÃ©curisÃ©e** : Les modÃ¨les sont mis Ã  jour de maniÃ¨re sÃ©curisÃ©e via Hugging Face.

    #### Mesures de SÃ©curitÃ© SupplÃ©mentaires
    - Nous utilisons des tokens d'authentification sÃ©curisÃ©s pour accÃ©der aux modÃ¨les.
    - Les fichiers YouTube sont tÃ©lÃ©chargÃ©s et traitÃ©s localement, sans stockage permanent.
    - Aucune donnÃ©e utilisateur n'est conservÃ©e aprÃ¨s la fermeture de la session.

    En utilisant Scribe, vous bÃ©nÃ©ficiez d'un traitement de donnÃ©es hautement sÃ©curisÃ© et respectueux de la vie privÃ©e, tout en profitant de la puissance des modÃ¨les d'IA de pointe.
    """)
    # ... (le reste du fichier reste inchangÃ©)    
    with gr.Tabs():
        with gr.Tab("Fichier audio ğŸ“"):
            gr.Markdown("### ğŸ“‚ Transcription de fichiers audio")
            audio_input = gr.Audio(type="filepath", label="Chargez votre fichier audio")
            task_input = gr.Radio(["transcribe", "translate"], label="Choisissez la tÃ¢che", value="transcribe")
            transcribe_button = gr.Button("ğŸš€ Lancer la transcription", elem_classes="button-primary")
            
            progress_display = gr.Markdown(label="Ã‰tat de la progression")
            
            with gr.Accordion("RÃ©sultats ğŸ“Š", open=True):
                raw_output = gr.Textbox(label="ğŸ“ Transcription brute", info="Texte gÃ©nÃ©rÃ© par le modÃ¨le. Modifiable si nÃ©cessaire.")
                speaker_output = gr.Textbox(label="ğŸ‘¥ Diarisation (format simplifiÃ©)", info="Identification des locuteurs. Format : 'SPEAKER_XX: texte'")
            with gr.Accordion("MÃ©tadonnÃ©es (optionnel) ğŸ“Œ", open=False):
                audio_duration = gr.Textbox(label="â±ï¸ DurÃ©e de l'audio (mm:ss)")
                location = gr.Textbox(label="ğŸ“ Lieu de l'enregistrement")
                speaker_age = gr.Number(label="ğŸ‘¤ Ã‚ge de l'intervenant principal")
                context = gr.Textbox(label="ğŸ“ Contexte de l'enregistrement")
            
            format_button = gr.Button("âœ¨ GÃ©nÃ©rer la transcription formatÃ©e", elem_classes="button-secondary")
            formatted_output = gr.Markdown(label="ğŸ“„ Transcription formatÃ©e :")


        with gr.Tab("Microphone ğŸ¤"):
            gr.Markdown("### ğŸ—£ï¸ Enregistrement et transcription en direct")
            mic_input = gr.Audio(type="filepath", label="Enregistrez votre voix")
            mic_task_input = gr.Radio(["transcribe", "translate"], label="Choisissez la tÃ¢che", value="transcribe")
            mic_transcribe_button = gr.Button("ğŸš€ Transcrire l'enregistrement", elem_classes="button-primary")
            
            mic_progress_display = gr.Markdown(label="Ã‰tat de la progression")
            
            with gr.Accordion("RÃ©sultats ğŸ“Š", open=True):
                mic_raw_output = gr.Textbox(label="ğŸ“ Transcription brute", info="Texte gÃ©nÃ©rÃ© par le modÃ¨le. Modifiable si nÃ©cessaire.")
                mic_speaker_output = gr.Textbox(label="ğŸ‘¥ Diarisation (format simplifiÃ©)", info="Identification des locuteurs. Format : 'SPEAKER_XX: texte'")
            with gr.Accordion("MÃ©tadonnÃ©es (optionnel) ğŸ“Œ", open=False):
                mic_audio_duration = gr.Textbox(label="â±ï¸ DurÃ©e de l'enregistrement (mm:ss)")
                mic_location = gr.Textbox(label="ğŸ“ Lieu de l'enregistrement")
                mic_speaker_age = gr.Number(label="ğŸ‘¤ Ã‚ge de l'intervenant principal")
                mic_context = gr.Textbox(label="ğŸ“ Contexte de l'enregistrement")
            
            mic_format_button = gr.Button("âœ¨ GÃ©nÃ©rer la transcription formatÃ©e", elem_classes="button-secondary")
            mic_formatted_output = gr.Markdown(label="ğŸ“„ Transcription formatÃ©e :")
            
        with gr.Tab("YouTube ğŸ¥"):
            gr.Markdown("### ğŸŒ Transcription Ã  partir de vidÃ©os YouTube")
            yt_input = gr.Textbox(lines=1, placeholder="Collez l'URL d'une vidÃ©o YouTube ici", label="ğŸ”— URL YouTube")
            yt_task_input = gr.Radio(["transcribe", "translate"], label="Choisissez la tÃ¢che", value="transcribe")
            yt_transcribe_button = gr.Button("ğŸš€ Transcrire la vidÃ©o", elem_classes="button-primary")
            
            yt_progress_display = gr.Markdown(label="Ã‰tat de la progression")
            
            yt_html_output = gr.HTML(label="â–¶ï¸ AperÃ§u de la vidÃ©o")
            
            with gr.Accordion("RÃ©sultats ğŸ“Š", open=True):
                yt_raw_output = gr.Textbox(label="ğŸ“ Transcription brute", info="Texte gÃ©nÃ©rÃ© par le modÃ¨le. Modifiable si nÃ©cessaire.")
                yt_speaker_output = gr.Textbox(label="ğŸ‘¥ Diarisation (format simplifiÃ©)", info="Identification des locuteurs. Format : 'SPEAKER_XX: texte'")
            with gr.Accordion("MÃ©tadonnÃ©es (optionnel) ğŸ“Œ", open=False):
                yt_audio_duration = gr.Textbox(label="â±ï¸ DurÃ©e de la vidÃ©o (mm:ss)")
                yt_channel = gr.Textbox(label="ğŸ“º Nom de la chaÃ®ne YouTube")
                yt_publish_date = gr.Textbox(label="ğŸ“… Date de publication")
                yt_context = gr.Textbox(label="ğŸ“ Contexte de la vidÃ©o")
            
            yt_format_button = gr.Button("âœ¨ GÃ©nÃ©rer la transcription formatÃ©e", elem_classes="button-secondary")
            yt_formatted_output = gr.Markdown(label="ğŸ“„ Transcription formatÃ©e :")


    gr.Markdown("""### ğŸ› ï¸ CapacitÃ©s :
    - Transcription multilingue avec dÃ©tection automatique de la langue
    - Traduction vers le franÃ§ais (pour les contenus non francophones)
    - Identification prÃ©cise des changements de locuteurs
    - Traitement de fichiers audio, enregistrements en direct et vidÃ©os YouTube
    - Gestion de divers formats audio et qualitÃ©s d'enregistrement

    """)
        
    with gr.Accordion("â“ README :", open=False):
        gr.Markdown("""
    - Concepteur : Woziii
    - ModÃ¨les :
        - [Whisper-mÃ©dium](https://huggingface.co/openai/whisper-medium) : Model size - 764M params - Tensor type F32 -
        - [speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1) : Model size - Unknow - Tensor type F32 -
    - Version : V.2.0.0-BÃªta
    - Langues : FR, EN
    - Copyright : cc-by-nc-4.0
    - [En savoir +](https://huggingface.co/spaces/Woziii/scribe/blob/main/README.md)
    """)

    # Connexions des boutons aux fonctions appropriÃ©es
    transcribe_button.click(
    process_transcription,
    inputs=[audio_input, task_input],
    outputs=[progress_display, raw_output, speaker_output]
    )

    format_button.click(
        format_to_markdown,
        inputs=[raw_output, speaker_output, audio_duration, location, speaker_age, context],
        outputs=formatted_output
    )

    mic_transcribe_button.click(
    process_transcription,
    inputs=[mic_input, mic_task_input],
    outputs=[mic_progress_display, mic_raw_output, mic_speaker_output]
    )

    mic_format_button.click(
        format_to_markdown,
        inputs=[mic_raw_output, mic_speaker_output, audio_duration, location, speaker_age, context],
        outputs=mic_formatted_output
    )

    yt_transcribe_button.click(
    process_yt_transcription,
    inputs=[yt_input, yt_task_input],
    outputs=[yt_html_output, yt_raw_output, yt_speaker_output]
    )

    yt_format_button.click(
        format_to_markdown,
        inputs=[yt_raw_output, yt_speaker_output, audio_duration, location, speaker_age, context],
        outputs=yt_formatted_output
    )
    

if __name__ == "__main__":
    demo.queue().launch()
