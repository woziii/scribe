import spaces
import torch
import gradio as gr
import yt_dlp as youtube_dl
from transformers import pipeline
from transformers.pipelines.audio_utils import ffmpeg_read
from transformers import WhisperProcessor
from pyannote.audio import Pipeline as PyannotePipeline
from datetime import datetime
import tempfile
import numpy as np
from itertools import groupby
from datetime import datetime
from gradio.components import State
import os
import re


    
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:307'

try:
    diarization_pipeline = PyannotePipeline.from_pretrained(
        "pyannote/speaker-diarization-3.1",
        use_auth_token=os.environ["HF_TOKEN"]
    )
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    diarization_pipeline.to(device)
except Exception as e:
    print(f"Error initializing diarization pipeline: {e}")
    diarization_pipeline = None

# Updated Whisper model
MODEL_NAME = "openai/whisper-medium"
#BATCH_SIZE = 2  # R√©duction de la taille du batch
FILE_LIMIT_MB = 1000
YT_LENGTH_LIMIT_S = 3600

device = 0 if torch.cuda.is_available() else "cpu"

pipe = pipeline(
    task="automatic-speech-recognition",
    model=MODEL_NAME,
    #chunk_length_s=30,
    device=device,
    model_kwargs={"low_cpu_mem_usage": True},
)




def associate_speakers_with_timestamps(transcription_result, diarization, tolerance=0.02, min_segment_duration=0.05):
    word_segments = transcription_result['chunks']
    diarization_segments = list(diarization.itertracks(yield_label=True))
    speaker_transcription = []
    current_speaker = None
    current_text = []
    last_word_end = 0

    def flush_current_segment():
        nonlocal current_speaker, current_text
        if current_speaker and current_text:
            speaker_transcription.append((current_speaker, ' '.join(current_text)))
            current_text = []

    for word in word_segments:
        word_start, word_end = word['timestamp']
        word_text = word['text']

        # Trouver le segment de diarisation correspondant
        matching_segment = None
        for segment, _, speaker in diarization_segments:
            if segment.start - tolerance <= word_start < segment.end + tolerance:
                matching_segment = (segment, speaker)
                break

        if matching_segment:
            segment, speaker = matching_segment
            if speaker != current_speaker:
                flush_current_segment()
                current_speaker = speaker

            # G√©rer les pauses longues
            if word_start - last_word_end > 1.0:  # Pause de plus d'une seconde
                flush_current_segment()

            current_text.append(word_text)
            last_word_end = word_end
        else:
            # Si aucun segment ne correspond, attribuer au dernier locuteur connu
            if current_speaker:
                current_text.append(word_text)
            else:
                # Si c'est le premier mot sans correspondance, cr√©er un nouveau segment
                current_speaker = "SPEAKER_UNKNOWN"
                current_text.append(word_text)

    flush_current_segment()

    # Fusionner les segments courts du m√™me locuteur
    merged_transcription = []
    for speaker, text in speaker_transcription:
        if not merged_transcription or merged_transcription[-1][0] != speaker or len(text.split()) > 3:
            merged_transcription.append((speaker, text))
        else:
            merged_transcription[-1] = (speaker, merged_transcription[-1][1] + " " + text)

    return merged_transcription
    
def simplify_diarization_output(speaker_transcription):
    simplified = []
    for speaker, text in speaker_transcription:
        simplified.append(f"{speaker}: {text}")
    return "\n".join(simplified)

def parse_simplified_diarization(simplified_text):
    pattern = r"(SPEAKER_\d+):\s*(.*)"
    matches = re.findall(pattern, simplified_text, re.MULTILINE)
    return [(speaker, text.strip()) for speaker, text in matches]

def process_transcription(*args):
    generator = transcribe_and_diarize(*args)
    for progress_message, raw_text, speaker_transcription in generator:
        pass  # Consommer le g√©n√©rateur jusqu'√† la fin
    simplified_diarization = simplify_diarization_output(speaker_transcription)
    return progress_message, raw_text, simplified_diarization

def process_yt_transcription(*args):
    html_embed, raw_text, speaker_transcription = yt_transcribe(*args)
    simplified_diarization = simplify_diarization_output(speaker_transcription)
    return html_embed, raw_text, simplified_diarization
    

# New functions for progress indicator
def create_progress_indicator():
    return gr.State({"stage": 0, "message": "En attente de d√©marrage..."})

def update_progress(progress_state, stage, message):
    progress_state["stage"] = stage
    progress_state["message"] = message
    return progress_state

def display_progress(progress_state):
    stages = [
        "Chargement du fichier",
        "Pr√©paration de l'audio",
        "Transcription en cours",
        "Diarisation (identification des locuteurs)",
        "Finalisation des r√©sultats"
    ]
    progress = (progress_state["stage"] / len(stages)) * 100
    return gr.HTML(f"""
        <style>
            @keyframes pulse {{
                0% {{ box-shadow: 0 0 0 0 rgba(66, 133, 244, 0.7); }}
                70% {{ box-shadow: 0 0 0 10px rgba(66, 133, 244, 0); }}
                100% {{ box-shadow: 0 0 0 0 rgba(66, 133, 244, 0); }}
            }}
        </style>
        <div style="margin-bottom: 20px; background-color: #f0f0f0; padding: 15px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
            <h4 style="margin-bottom: 10px; color: #333; font-weight: bold;">Progression de la transcription</h4>
            <div style="background-color: #e0e0e0; height: 24px; border-radius: 12px; overflow: hidden; position: relative;">
                <div style="background-color: #4285F4; width: {progress}%; height: 100%; border-radius: 12px; transition: width 0.5s ease-in-out;"></div>
                <div style="position: absolute; top: 0; left: 0; right: 0; bottom: 0; display: flex; align-items: center; justify-content: center; color: #333; font-weight: bold;">
                    {progress:.0f}%
                </div>
            </div>
            <p style="margin-top: 10px; color: #666; font-style: italic;">{progress_state["message"]}</p>
            <div style="width: 10px; height: 10px; background-color: #4285F4; border-radius: 50%; margin-top: 10px; animation: pulse 2s infinite;"></div>
        </div>
    """)
    
@spaces.GPU(duration=120)
def transcribe_and_diarize(file_path, task, progress=gr.Progress()):
    progress(0, desc="Initialisation...")
    yield "Chargement du fichier...", None, None

    progress(0.2, desc="Pr√©paration de l'audio...")
    yield "Pr√©paration de l'audio...", None, None

    progress(0.4, desc="Laissez moi quelques minutes pour d√©chiffrer les voix et r√©diger l'audio ü§ì ‚úçÔ∏è ...")
    transcription_result = pipe(file_path, generate_kwargs={"task": task, "language": "fr"}, return_timestamps="word")
    yield "Transcription en cours...", None, None

    progress(0.6, desc=" C'est fait üòÆ‚Äçüí® ! Je m'active √† fusionner tout √ßa, un instant, J'y suis presque...")
    if diarization_pipeline:
        diarization = diarization_pipeline(file_path)
        speaker_transcription = associate_speakers_with_timestamps(transcription_result, diarization)
    else:
        speaker_transcription = [(None, transcription_result['text'])]
    yield "Diarisation en cours...", None, None

    progress(0.8, desc="Finalisation des r√©sultats...")
    yield "Voil√†!", transcription_result['text'], speaker_transcription

    progress(1.0, desc="Termin√©!")
    return "Transcription termin√©e!", transcription_result['text'], speaker_transcription

def format_to_markdown(transcription_text, speaker_transcription, audio_duration=None, location=None, speaker_age=None, context=None):
    metadata = {
        "Date de traitement": datetime.now().strftime('%d/%m/%Y %H:%M'),
        "Dur√©e de l'audio": f"{audio_duration} secondes" if audio_duration else "[√† remplir]",
        "Lieu": location if location else "[non sp√©cifi√©]",
        "√Çge de l'intervenant": f"{speaker_age} ans" if speaker_age else "[non sp√©cifi√©]",
        "Contexte": context if context else "[non sp√©cifi√©]"
    }
    
    metadata_text = "\n".join([f"- **{key}** : '{value}'" for key, value in metadata.items()])
    
    try:
        if isinstance(speaker_transcription, str):
            speaker_transcription = parse_simplified_diarization(speaker_transcription)
        
        if isinstance(speaker_transcription, list) and all(isinstance(item, tuple) and len(item) == 2 for item in speaker_transcription):
            formatted_transcription = []
            for speaker, text in speaker_transcription:
                formatted_transcription.append(f"**{speaker}**: {text}")
            transcription_text = "\n\n".join(formatted_transcription)
        else:
            raise ValueError("Invalid speaker transcription format")
    except Exception as e:
        print(f"Error formatting speaker transcription: {e}")
        transcription_text = "Error formatting speaker transcription. Using raw transcription instead.\n\n" + transcription_text

    formatted_output = f"""
# Transcription Format√©e

## M√©tadonn√©es
{metadata_text}

## Transcription
{transcription_text}
"""
    return formatted_output

def _return_yt_html_embed(yt_url):
    video_id = yt_url.split("?v=")[-1]
    HTML_str = (
        f'<center> <iframe width="500" height="320" src="https://www.youtube.com/embed/{video_id}"> </iframe>'
        " </center>"
    )
    return HTML_str

def download_yt_audio(yt_url, filename):
    info_loader = youtube_dl.YoutubeDL()
    
    try:
        info = info_loader.extract_info(yt_url, download=False)
    except youtube_dl.utils.DownloadError as err:
        raise gr.Error(str(err))
    
    file_length = info["duration"]
    
    if file_length > YT_LENGTH_LIMIT_S:
        yt_length_limit_hms = time.strftime("%H:%M:%S", time.gmtime(YT_LENGTH_LIMIT_S))
        file_length_hms = time.strftime("%H:%M:%S", time.gmtime(file_length))
        raise gr.Error(f"La dur√©e maximale YouTube est de {yt_length_limit_hms}, la vid√©o YouTube dure {file_length_hms}.")
    
    ydl_opts = {"outtmpl": filename, "format": "bestaudio/best"}
    
    with youtube_dl.YoutubeDL(ydl_opts) as ydl:
        try:
            ydl.download([yt_url])
        except youtube_dl.utils.ExtractorError as err:
            raise gr.Error(str(err))

@spaces.GPU(duration=120)
def yt_transcribe(yt_url, task):
    html_embed_str = _return_yt_html_embed(yt_url)

    with tempfile.TemporaryDirectory() as tmpdirname:
        filepath = os.path.join(tmpdirname, "video.mp4")
        download_yt_audio(yt_url, filepath)
        with open(filepath, "rb") as f:
            inputs = f.read()

    inputs = ffmpeg_read(inputs, pipe.feature_extractor.sampling_rate)
    inputs = {"array": inputs, "sampling_rate": pipe.feature_extractor.sampling_rate}

    transcription_result = pipe(inputs, generate_kwargs={"task": task}, return_timestamps=True)
    transcription_text = transcription_result['text']

    if diarization_pipeline:
        diarization = diarization_pipeline(filepath)
        speaker_transcription = associate_speakers_with_timestamps(transcription_result, diarization)
    else:
        speaker_transcription = [(None, transcription_text)]

    return html_embed_str, transcription_text, speaker_transcription

def create_info_box(title, content):
    return gr.Markdown(f"### {title}\n{content}")

css_file_path = os.path.join(os.path.dirname(__file__), "style.css")
with open(css_file_path, "r") as f:
    custom_css = f.read()

theme = gr.themes.Default().set(
    body_background_fill="#f0f2f5",
    body_background_fill_dark="#2c3e50",
    button_primary_background_fill="#3498db",
    button_primary_background_fill_dark="#2980b9",
    button_secondary_background_fill="#2ecc71",
    button_secondary_background_fill_dark="#27ae60",
)

demo = gr.Blocks(
    theme=gr.themes.Default(),
    title="Scribe - Assistant de Transcription Audio üéôÔ∏èüìù",
    css=custom_css
)


with demo:
    gr.Markdown("""# üéôÔ∏è **Scribe** : L'assistant de Transcription Audio Intelligent üìù 
    ### ‚ö†Ô∏è Cette version est une maquette publique. Ne pas mettre de donn√©es sensibles, priv√©es ou confidentielles. ‚ö†Ô∏è""")
    gr.HTML(
        """
        <div class="logo">
            <img src="https://image.noelshack.com/fichiers/2024/33/4/1723713257-dbe58773-0638-445b-a88c-3fc1f2002408.jpg" alt="Scribe Logo">
        </div>
        """
    )
    gr.Markdown("## **Bienvenue sur Scribe, une solution pour la transcription audio s√©curis√©e. Transformez efficacement vos fichiers audio, enregistrements en direct ou vid√©os YouTube en texte pr√©cis.**")

    gr.Markdown("""
    ### üîç **Fonctionnement du Mod√®le** :
    Scribe utilise une approche en deux √©tapes pour transformer l'audio en texte structur√© :

    1. **Transcription avec Whisper Medium** :
       - Mod√®le de reconnaissance vocale d√©velopp√© par OpenAI
       - Utilise un r√©seau neuronal encodeur-d√©codeur avec attention
       - Capable de traiter divers accents et bruits de fond
       - Optimis√© pour un √©quilibre entre pr√©cision et rapidit√©

    2. **Diarisation avec pyannote/speaker-diarization-3.1** :
       - Identifie et segmente les diff√©rents locuteurs dans l'audio
       - Utilise des techniques d'apprentissage profond pour l'extraction de caract√©ristiques vocales
       - Applique un algorithme de clustering pour regrouper les segments par locuteur



    ### üí° **Conseils pour de Meilleurs R√©sultats**
    - Utilisez des enregistrements de haute qualit√© avec peu de bruit de fond.
    - Pour les longs enregistrements, il est recommand√© de segmenter votre audio.
    - V√©rifiez toujours la transcription, en particulier pour les termes techniques ou les noms propres.
    - Utilisez des microphones externes pour les enregistrements en direct si possible.

    ### ‚öôÔ∏è Sp√©cifications Techniques :
    - Mod√®le de transcription : Whisper Medium
    - Pipeline de diarisation : pyannote/speaker-diarization-3.1
    - Limite de taille de fichier : _(Nous n'avons, √† ce jour, pas de limite pr√©cise. Cependant, **nous vous recommandons de ne pas d√©passer 5 minutes.** )_
    - Dur√©e maximale pour les vid√©os YouTube : _(Nous n'avons, √† ce jour, pas de limite pr√©cise. Cependant, pour une utilisation optimale, l'audio ne doit pas d√©passer 30 minutes. )_
    - Formats audio support√©s : MP3, WAV, M4A, et plus
    """)
    with gr.Accordion("üîê S√©curit√© des Donn√©es et Pipelines", open=False):
        gr.Markdown("""

    #### Qu'est-ce qu'une pipeline ?
    Une pipeline dans le contexte de l'apprentissage automatique est une s√©rie d'√©tapes de traitement des donn√©es, allant de l'entr√©e brute √† la sortie finale. Dans Scribe, nous utilisons deux pipelines principales :

    1. **Pipeline de Transcription** : Bas√©e sur le mod√®le Whisper Medium, elle convertit l'audio en texte.
    2. **Pipeline de Diarisation** : Identifie les diff√©rents locuteurs dans l'audio.

    #### Comment fonctionnent nos pipelines ?
    1. **Chargement Local** : Les mod√®les sont charg√©s localement sur votre machine ou serveur.
    2. **Traitement In-Situ** : Toutes les donn√©es sont trait√©es sur place, sans envoi √† des serveurs externes.
    3. **M√©moire Volatile** : Les donn√©es sont stock√©es temporairement en m√©moire vive et effac√©es apr√®s utilisation.

    #### S√©curit√© et Confidentialit√©
    - **Pas de Transmission Externe** : Vos donn√©es audio et texte restent sur votre syst√®me local.
    - **Isolation** : Chaque session utilisateur est isol√©e des autres.
    - **Nettoyage Automatique** : Les fichiers temporaires sont supprim√©s apr√®s chaque utilisation.
    - **Mise √† Jour S√©curis√©e** : Les mod√®les sont mis √† jour de mani√®re s√©curis√©e via Hugging Face.

    #### Mesures de S√©curit√© Suppl√©mentaires
    - Nous utilisons des tokens d'authentification s√©curis√©s pour acc√©der aux mod√®les.
    - Les fichiers YouTube sont t√©l√©charg√©s et trait√©s localement, sans stockage permanent.
    - Aucune donn√©e utilisateur n'est conserv√©e apr√®s la fermeture de la session.

    En utilisant Scribe, vous b√©n√©ficiez d'un traitement de donn√©es hautement s√©curis√© et respectueux de la vie priv√©e, tout en profitant de la puissance des mod√®les d'IA de pointe.
    """)
    # ... (le reste du fichier reste inchang√©)    
    with gr.Tabs():
        with gr.Tab("Fichier audio üìÅ"):
            gr.Markdown("### üìÇ Transcription de fichiers audio")
            audio_input = gr.Audio(type="filepath", label="Chargez votre fichier audio")
            task_input = gr.Radio(["transcribe", "translate"], label="Choisissez la t√¢che", value="transcribe")
            transcribe_button = gr.Button("üöÄ Lancer la transcription", elem_classes="button-primary")
            
            progress_display = gr.Markdown(label="√âtat de la progression")
            
            with gr.Accordion("R√©sultats üìä", open=True):
                raw_output = gr.Textbox(label="üìù Transcription brute", info="Texte g√©n√©r√© par le mod√®le. Modifiable si n√©cessaire.")
                speaker_output = gr.Textbox(label="üë• Diarisation (format simplifi√©)", info="Identification des locuteurs. Format : 'SPEAKER_XX: texte'")
            with gr.Accordion("M√©tadonn√©es (optionnel) üìå", open=False):
                audio_duration = gr.Textbox(label="‚è±Ô∏è Dur√©e de l'audio (mm:ss)")
                location = gr.Textbox(label="üìç Lieu de l'enregistrement")
                speaker_age = gr.Number(label="üë§ √Çge de l'intervenant principal")
                context = gr.Textbox(label="üìù Contexte de l'enregistrement")
            
            format_button = gr.Button("‚ú® G√©n√©rer la transcription format√©e", elem_classes="button-secondary")
            formatted_output = gr.Markdown(label="üìÑ Transcription format√©e :")


        with gr.Tab("Microphone üé§"):
            gr.Markdown("### üó£Ô∏è Enregistrement et transcription en direct")
            mic_input = gr.Audio(type="filepath", label="Enregistrez votre voix")
            mic_task_input = gr.Radio(["transcribe", "translate"], label="Choisissez la t√¢che", value="transcribe")
            mic_transcribe_button = gr.Button("üöÄ Transcrire l'enregistrement", elem_classes="button-primary")
            
            mic_progress_display = gr.Markdown(label="√âtat de la progression")
            
            with gr.Accordion("R√©sultats üìä", open=True):
                mic_raw_output = gr.Textbox(label="üìù Transcription brute", info="Texte g√©n√©r√© par le mod√®le. Modifiable si n√©cessaire.")
                mic_speaker_output = gr.Textbox(label="üë• Diarisation (format simplifi√©)", info="Identification des locuteurs. Format : 'SPEAKER_XX: texte'")
            with gr.Accordion("M√©tadonn√©es (optionnel) üìå", open=False):
                mic_audio_duration = gr.Textbox(label="‚è±Ô∏è Dur√©e de l'enregistrement (mm:ss)")
                mic_location = gr.Textbox(label="üìç Lieu de l'enregistrement")
                mic_speaker_age = gr.Number(label="üë§ √Çge de l'intervenant principal")
                mic_context = gr.Textbox(label="üìù Contexte de l'enregistrement")
            
            mic_format_button = gr.Button("‚ú® G√©n√©rer la transcription format√©e", elem_classes="button-secondary")
            mic_formatted_output = gr.Markdown(label="üìÑ Transcription format√©e :")
            
        with gr.Tab("YouTube üé•"):
            gr.Markdown("### üåê Transcription √† partir de vid√©os YouTube")
            yt_input = gr.Textbox(lines=1, placeholder="Collez l'URL d'une vid√©o YouTube ici", label="üîó URL YouTube")
            yt_task_input = gr.Radio(["transcribe", "translate"], label="Choisissez la t√¢che", value="transcribe")
            yt_transcribe_button = gr.Button("üöÄ Transcrire la vid√©o", elem_classes="button-primary")
            
            yt_progress_display = gr.Markdown(label="√âtat de la progression")
            
            yt_html_output = gr.HTML(label="‚ñ∂Ô∏è Aper√ßu de la vid√©o")
            
            with gr.Accordion("R√©sultats üìä", open=True):
                yt_raw_output = gr.Textbox(label="üìù Transcription brute", info="Texte g√©n√©r√© par le mod√®le. Modifiable si n√©cessaire.")
                yt_speaker_output = gr.Textbox(label="üë• Diarisation (format simplifi√©)", info="Identification des locuteurs. Format : 'SPEAKER_XX: texte'")
            with gr.Accordion("M√©tadonn√©es (optionnel) üìå", open=False):
                yt_audio_duration = gr.Textbox(label="‚è±Ô∏è Dur√©e de la vid√©o (mm:ss)")
                yt_channel = gr.Textbox(label="üì∫ Nom de la cha√Æne YouTube")
                yt_publish_date = gr.Textbox(label="üìÖ Date de publication")
                yt_context = gr.Textbox(label="üìù Contexte de la vid√©o")
            
            yt_format_button = gr.Button("‚ú® G√©n√©rer la transcription format√©e", elem_classes="button-secondary")
            yt_formatted_output = gr.Markdown(label="üìÑ Transcription format√©e :")


    gr.Markdown("""### üõ†Ô∏è Capacit√©s :
    - Transcription multilingue avec d√©tection automatique de la langue
    - Traduction vers le fran√ßais (pour les contenus non francophones)
    - Identification pr√©cise des changements de locuteurs
    - Traitement de fichiers audio, enregistrements en direct et vid√©os YouTube
    - Gestion de divers formats audio et qualit√©s d'enregistrement

    """)
        
    with gr.Accordion("‚ùì README :", open=False):
        gr.Markdown("""
    - Concepteur : Woziii
    - Mod√®les :
        - [Whisper-m√©dium](https://huggingface.co/openai/whisper-medium) : Model size - 764M params - Tensor type F32 -
        - [speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1) : Model size - Unknow - Tensor type F32 -
    - Version : V.2.0.0-B√™ta
    - Langues : FR, EN
    - Copyright : cc-by-nc-4.0
    - [En savoir +](https://huggingface.co/spaces/Woziii/scribe/blob/main/README.md)
    """)

    # Connexions des boutons aux fonctions appropri√©es
    transcribe_button.click(
    process_transcription,
    inputs=[audio_input, task_input],
    outputs=[progress_display, raw_output, speaker_output]
    )

    format_button.click(
        format_to_markdown,
        inputs=[raw_output, speaker_output, audio_duration, location, speaker_age, context],
        outputs=formatted_output
    )

    mic_transcribe_button.click(
    process_transcription,
    inputs=[mic_input, mic_task_input],
    outputs=[mic_progress_display, mic_raw_output, mic_speaker_output]
    )

    mic_format_button.click(
        format_to_markdown,
        inputs=[mic_raw_output, mic_speaker_output, audio_duration, location, speaker_age, context],
        outputs=mic_formatted_output
    )

    yt_transcribe_button.click(
    process_yt_transcription,
    inputs=[yt_input, yt_task_input],
    outputs=[yt_html_output, yt_raw_output, yt_speaker_output]
    )

    yt_format_button.click(
        format_to_markdown,
        inputs=[yt_raw_output, yt_speaker_output, audio_duration, location, speaker_age, context],
        outputs=yt_formatted_output
    )
    

if __name__ == "__main__":
    demo.queue().launch()
